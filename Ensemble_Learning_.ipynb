{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Ensemble Learning Assignment"
      ],
      "metadata": {
        "id": "En2KiF4iuRvm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "\n",
        "\n",
        "- Ensemble Learning in machine learning is a technique where multiple models (often called base learners or weak learners) are combined to make better predictions than any single model could on its own.\n",
        "\n",
        "- The main idea is that a group of weak models working together can outperform a single strong model, as combining multiple perspectives helps reduce errors, variance, and bias.\n",
        "\n",
        "- How It Works:\n",
        "\n",
        "  - Each model in the ensemble makes a prediction.\n",
        "\n",
        "  - The predictions are then combined using methods like:\n",
        "\n",
        "     - Voting (for classification) — majority wins.\n",
        "\n",
        "     - Averaging (for regression) — mean of all predictions.\n",
        "\n",
        "     - Weighted combination — some models have more influence based on their accuracy.\n",
        "\n",
        "- Advantages:\n",
        "\n",
        "   - Increases accuracy and robustness of predictions.\n",
        "\n",
        "   - Reduces overfitting and variance.\n",
        "\n",
        "   - Works well even when individual models are weak.\n",
        "\n",
        "Common Ensemble Methods:\n",
        "\n",
        "   - Bagging (Bootstrap Aggregating) – e.g., Random Forest\n",
        "\n",
        "   - Boosting – e.g., AdaBoost, Gradient Boosting, XGBoost\n",
        "\n",
        "   - Stacking – combines predictions of multiple models using another model (meta-learner).     \n",
        "\n",
        "\n",
        "#Question 2: What is the difference between Bagging and Boosting?\n",
        "\n",
        "- Both Bagging and Boosting are ensemble learning techniques that combine multiple models to improve performance — but they differ in how the models are trained and combined.\n",
        "\n",
        "1. Goal:\n",
        "\n",
        "   -  Bagging aims to reduce variance and prevent overfitting.\n",
        "\n",
        "   - Boosting aims to reduce bias and improve weak models.\n",
        "\n",
        "2. Training Method:\n",
        "\n",
        "   - Bagging trains all models independently and in parallel.\n",
        "\n",
        "   - Boosting trains models sequentially, each new model learning from previous errors.\n",
        "\n",
        "3. Data Sampling:\n",
        "\n",
        "   - Bagging uses bootstrapped samples (sampling with replacement).\n",
        "\n",
        "   - Boosting uses the entire dataset, but increases the weight of misclassified samples.\n",
        "\n",
        "4. Error Handling:\n",
        "\n",
        "   - Bagging treats all data points equally.\n",
        "\n",
        "   - Boosting focuses more on difficult or misclassified points.\n",
        "\n",
        "5. Combination Method:\n",
        "\n",
        "   - Bagging combines outputs by majority voting (classification) or averaging (regression).\n",
        "\n",
        "   - Boosting combines models using a weighted sum based on model accuracy.\n",
        "\n",
        "6. Overfitting:\n",
        "\n",
        "   - Bagging is less prone to overfitting.\n",
        "\n",
        "   - Boosting can overfit if too many weak learners are added\n",
        "\n",
        "#Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "\n",
        "- Bootstrap sampling is a statistical technique where we create multiple random samples from the original dataset with replacement.\n",
        "\n",
        "- This means that some data points may appear more than once in a sample, while others may not appear at all.\n",
        "\n",
        "- Each sample (called a bootstrap sample) is typically the same size as the original dataset.\n",
        "\n",
        "__ Role of Bootstrap Sampling in Bagging (e.g., Random Forest):\n",
        "\n",
        "1. Creates Diversity:\n",
        "    - Each model (e.g., each decision tree in a Random Forest) is trained on a different bootstrap sample, which makes the models slightly different from one another.\n",
        "\n",
        "2. Reduces Variance:\n",
        "   - By combining multiple diverse models trained on different samples, Bagging reduces the overall variance and improves prediction stability.\n",
        "\n",
        "3. Improves Robustness:\n",
        "   - Since each model sees a slightly different version of the data, the final ensemble model becomes more robust and less sensitive to noise or outliers.\n",
        "4. Enables Parallel Training:\n",
        "     - Each bootstrap sample allows models to be trained independently and in parallel, making Bagging computationally efficient.\n",
        "\n",
        "\n",
        "#Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "\n",
        "\n",
        "- In Bagging (like in Random Forest), each model is trained on a bootstrap sample created by sampling with replacement from the original dataset.\n",
        "\n",
        "   - Because sampling is done with replacement, about 63% of the data points are selected in each bootstrap sample.\n",
        "\n",
        "  - The remaining = 37% of the data that are not included in that sample are called Out-of-Bag (OOB) samples.\n",
        "\n",
        "- How OOB Samples Are Used:\n",
        "\n",
        "1. Model Evaluation Without a Test Set:\n",
        "    - Each model can be evaluated on its OOB samples, i.e., the data it has not seen during training.\n",
        "    - This gives an unbiased estimate of model performance.\n",
        "\n",
        "2. OOB Score Calculation:\n",
        "\n",
        "   - For each observation, predictions are made using only the models that did not include that observation in their training data.\n",
        "\n",
        "   - These predictions are aggregated (by majority vote or averaging).\n",
        "\n",
        "   - The overall OOB score is then computed as the accuracy (for classification) or R² score (for regression) based on these predictions.\n",
        "\n",
        "3. No Need for Cross-Validation:\n",
        "   - OOB evaluation acts as a built-in cross-validation method for Bagging models, saving computation time.  \n",
        "\n",
        "\n",
        "#Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        "\n",
        "1. Basis of Calculation:\n",
        "\n",
        "   - Decision Tree  --  Importance is based on how much each feature reduces impurity (like Gini or entropy) within that single tree.\n",
        "\n",
        "   - Random Forest-- Importance is the average impurity reduction for each feature across all trees in the forest.\n",
        "\n",
        "2. Stability:\n",
        "\n",
        "    - Decision Tree-- Can be unstable — small changes in data may lead to different feature importances.\n",
        "\n",
        "   - Random Forest -- Much more stable, since it averages results from many trees.\n",
        "\n",
        "3. Bias:\n",
        "\n",
        "    - Decision Tree --  Often biased toward features with many unique values.\n",
        "\n",
        "   -  Random Forest --  Reduces this bias through aggregation over multiple trees.\n",
        "\n",
        "4. Interpretability:\n",
        "\n",
        "   - Decision Tree --  Easier to interpret since there’s only one tree.\n",
        "\n",
        "   - Random Forest -- Harder to interpret but gives more accurate and reliable importance values.\n",
        "\n",
        "5. Accuracy of Importance Estimation:\n",
        "\n",
        "   - Decision Tree -- May overfit and give misleading importance for noisy features.\n",
        "\n",
        "   - Random Forest -- Provides robust, generalizable feature importance by averaging over multiple models.\n",
        "\n",
        "\n",
        "\n",
        "#Question 6: Write a Python program to:\n",
        "#● Load the Breast Cancer dataset using\n",
        "#sklearn.datasets.load_breast_cancer()\n",
        "#● Train a Random Forest Classifier\n",
        "#● Print the top 5 most important features based on feature importance scores.   \n",
        "\n"
      ],
      "metadata": {
        "id": "OAFa9QSBudGe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EeCRPsRuQvp",
        "outputId": "c9cc102d-8f48-4e35-bdac-210bdc3487a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "data = load_breast_cancer()\n",
        "\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "importances = model.feature_importances_\n",
        "\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': data.feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(feature_importance_df.head(5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 7: Write a Python program to:\n",
        "#● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "#● Evaluate its accuracy and compare with a single Decision Tree\n"
      ],
      "metadata": {
        "id": "o-hc88JPy8kf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a single Decision Tree classifier\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "y_pred_dt = dt_model.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "# Train a Bagging Classifier with Decision Trees (use 'estimator' instead of 'base_estimator')\n",
        "bagging_model = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_model.fit(X_train, y_train)\n",
        "y_pred_bag = bagging_model.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, y_pred_bag)\n",
        "\n",
        "# Print and compare accuracies\n",
        "print(\"Accuracy of Single Decision Tree:\", round(dt_accuracy, 3))\n",
        "print(\"Accuracy of Bagging Classifier:\", round(bagging_accuracy, 3))\n",
        "\n",
        "if bagging_accuracy > dt_accuracy:\n",
        "    print(\"\\n Bagging Classifier performed better due to reduced variance.\")\n",
        "else:\n",
        "    print(\"\\n Single Decision Tree performed equally or better in this run.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEsa8gCrzTno",
        "outputId": "a09f1698-7ce2-4c35-8b75-54ed37718925"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Single Decision Tree: 1.0\n",
            "Accuracy of Bagging Classifier: 1.0\n",
            "\n",
            " Single Decision Tree performed equally or better in this run.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 8: Write a Python program to:\n",
        "#● Train a Random Forest Classifier\n",
        "#● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "#● Print the best parameters and final accuracy\n"
      ],
      "metadata": {
        "id": "NvaU7o34ziM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [3, 5, 7, None]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Final Accuracy on Test Data:\", round(accuracy, 3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTv41oY-zhBS",
        "outputId": "37691471-37d9-45eb-ff93-f78a8f3dc933"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 3, 'n_estimators': 150}\n",
            "Final Accuracy on Test Data: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 9: Write a Python program to:\n",
        "#● Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
        "#● Compare their Mean Squared Errors (MSE)"
      ],
      "metadata": {
        "id": "Z4_BGeyV0JGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "bagging_regressor = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "y_pred_bag = bagging_regressor.predict(X_test)\n",
        "\n",
        "rf_regressor = RandomForestRegressor(\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "y_pred_rf = rf_regressor.predict(X_test)\n",
        "\n",
        "mse_bag = mean_squared_error(y_test, y_pred_bag)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "print(\"Mean Squared Error (Bagging Regressor):\", round(mse_bag, 3))\n",
        "print(\"Mean Squared Error (Random Forest Regressor):\", round(mse_rf, 3))\n",
        "\n",
        "if mse_rf < mse_bag:\n",
        "    print(\"\\n Random Forest Regressor performed better (lower MSE).\")\n",
        "else:\n",
        "    print(\"\\n Bagging Regressor performed equally or better in this run.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exz_pca_z6d3",
        "outputId": "5c2fd846-5a7e-4a8b-c535-5f77aa4fcf01"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (Bagging Regressor): 0.258\n",
            "Mean Squared Error (Random Forest Regressor): 0.258\n",
            "\n",
            " Random Forest Regressor performed better (lower MSE).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 10: You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data. You decide to use ensemble techniques to increase model performance.\n",
        "- Explain your step-by-step approach to:\n",
        " -  Choose between Bagging or Boosting\n",
        "- Handle overfitting\n",
        "- Select base models\n",
        "- Evaluate performance using cross-validation\n",
        "- Justify how ensemble learning improves decision-making in this real-world\n",
        "context."
      ],
      "metadata": {
        "id": "-duWj1BL0kGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans 10 -- 1. Choosing Between Bagging and Boosting:\n",
        "\n",
        "   - Use Bagging (e.g., Random Forest) if the model has high variance.\n",
        "\n",
        "   - Use Boosting (e.g., XGBoost) if the model has high bias and needs better accuracy.\n",
        "\n",
        "2. Handling Overfitting:\n",
        "\n",
        "    - Use cross-validation, regularization (e.g., learning rate, max_depth), and limit number of trees.\n",
        "\n",
        "3. Selecting Base Models:\n",
        "\n",
        "   - Decision Trees are used as base models since they handle nonlinearity and categorical data well.\n",
        "\n",
        "4. Evaluating Performance:\n",
        "\n",
        "   - Apply k-fold cross-validation to compare models using metrics like accuracy, precision, recall, and AUC.\n",
        "\n",
        "5. Justification:\n",
        "\n",
        "    - Ensemble learning combines multiple weak models to reduce errors, improve prediction stability, and provide more reliable loan default decisions."
      ],
      "metadata": {
        "id": "-FNmsPdJ0_vf"
      }
    }
  ]
}