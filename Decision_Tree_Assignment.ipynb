{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Decision Tree Assignment"
      ],
      "metadata": {
        "id": "L7mWkzTaXqGT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 1: What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "- A Decision Tree works like a flowchart — it splits data into branches based on feature values to reach a final decision (output class) at the leaf nodes.\n",
        "\n",
        "- How It Works in Classification:\n",
        "\n",
        "   1. Root Node: The tree starts with a root node that contains the entire dataset.\n",
        "\n",
        "   2. Splitting: The algorithm selects the best feature to split the data based on a criterion like:\n",
        "\n",
        "     - Gini Impurity\n",
        "\n",
        "     - Entropy / Information Gain\n",
        "\n",
        "   3. Decision Nodes: Each internal node represents a test on a feature (e.g., “Age > 30?”).\n",
        "\n",
        "   4. Branches: Each branch represents the outcome of that test (e.g., “Yes” or “No”).\n",
        "\n",
        "   5. Leaf Nodes: The end of each path is a leaf node that represents a class label (e.g., “Approved” or “Denied”).\n",
        "\n",
        "- Advantages:\n",
        "\n",
        "     - Easy to visualize and interpret\n",
        "\n",
        "    - Handles both numerical and categorical data\n",
        "\n",
        "    - No need for feature scaling\n",
        "\n",
        "- Disadvantages:\n",
        "\n",
        "   - Prone to overfitting\n",
        "\n",
        "   - Small data changes can lead to a different tree\n",
        "\n",
        "#Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "\n",
        "\n",
        "- Both Gini Impurity and Entropy are measures of impurity (or disorder) used to decide how to split data at each node in a Decision Tree.\n",
        "\n",
        "1. Gini Impurity\n",
        "\n",
        "  - Definition: Gini Impurity measures the probability that a randomly chosen sample would be incorrectly classified if it were labeled according to the class distribution in that node.\n",
        "\n",
        "  - Interpretation:\n",
        "\n",
        "      - Gini = 0 → Node is pure (all samples belong to one class).\n",
        "\n",
        "      - Higher Gini → More mixed classes (higher impurity).\n",
        "\n",
        "2. Entropy (Information Gain)\n",
        "\n",
        "  - Definition: Entropy measures the amount of randomness or uncertainty in the data.\n",
        "\n",
        "  - Interpretation:\n",
        "\n",
        "     - Entropy = 0 → Node is pure (all samples same class).\n",
        "\n",
        "     - Entropy = 1 → Classes are evenly mixed (50-50).\n",
        "\n",
        "- How They Impact Splits\n",
        "\n",
        "   - At each node, the Decision Tree algorithm tries different splits on features.\n",
        "\n",
        "   - It calculates Gini or Entropy for each possible split.\n",
        "\n",
        "   - The split that gives the lowest impurity (or highest Information Gain) is chosen.\n",
        "\n",
        "#Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "\n",
        "- Decision Trees often overfit — they become too complex and perform well on training data but poorly on unseen data.\n",
        "To control this, we use pruning, which simplifies the tree.\n",
        "\n",
        "1. Pre-Pruning (Early Stopping)\n",
        "\n",
        "   - Definition: Pre-pruning stops the tree from growing too deep by applying certain stopping conditions during the training process.\n",
        "\n",
        "   - Common Stopping Conditions:\n",
        "\n",
        "      - Maximum depth of the tree (max_depth)\n",
        "\n",
        "      - Minimum number of samples required to split a node (min_samples_split)\n",
        "\n",
        "      - Minimum impurity decrease required for a split\n",
        "\n",
        "      - Minimum samples per leaf\n",
        "\n",
        "   - Example: Stop splitting a node if it has fewer than 5 samples.\n",
        "\n",
        "2. Post-Pruning (Cost Complexity Pruning)\n",
        "\n",
        "   - Definition: Post-pruning allows the tree to grow fully first, and then removes unnecessary branches that do not improve model performance on a validation set.\n",
        "\n",
        "   - How It Works:\n",
        "\n",
        "     - Build a full decision tree.\n",
        "\n",
        "     - Evaluate subtrees on validation data.\n",
        "\n",
        "     - Prune (remove) branches that provide little or no accuracy gain.\n",
        "\n",
        "   - Example: CART algorithm uses cost complexity pruning — balancing accuracy vs. complexity.\n",
        "\n",
        "\n",
        "#Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "\n",
        "- Definition:-\n",
        "\n",
        "    - Information Gain (IG) measures how much uncertainty (impurity) in the  data is reduced after a dataset is split based on a feature.\n",
        "    - It helps the Decision Tree decide which feature and threshold to split on at each node.\n",
        "\n",
        "- Intuition:\n",
        "\n",
        "  - A good split separates the data so that each child node is purer (contains mostly one class).\n",
        "\n",
        "  - The greater the reduction in impurity (i.e., higher Information Gain), the better the split.\n",
        "\n",
        "- Why It’s Important:\n",
        "\n",
        "    - Guides the Tree: It helps the Decision Tree algorithm choose the best feature and threshold for splitting.\n",
        "\n",
        "    - Reduces Disorder: Maximizing Information Gain ensures that child nodes are more homogeneous, leading to clearer class separation.\n",
        "\n",
        "    - Improves Accuracy: Trees with high IG splits usually make more accurate predictions.\n",
        "\n",
        "\n",
        "#Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "\n",
        "- Real-World Applications\n",
        "\n",
        "   1. Business & Finance\n",
        "\n",
        "       - Credit Scoring: Predict whether a customer will repay a loan.\n",
        "\n",
        "       - Customer Churn: Identify customers likely to leave a service.\n",
        "\n",
        "  2. Healthcare\n",
        "\n",
        "      - Disease Diagnosis: Classify patients based on symptoms or test results.\n",
        "\n",
        "      - Treatment Decisions: Suggest treatments based on patient conditions.\n",
        "\n",
        "  3. Marketing\n",
        "\n",
        "      - Targeted Advertising: Segment customers for personalized ads.\n",
        "\n",
        "     - Sales Prediction: Predict which products a customer might buy.\n",
        "\n",
        "  4. Education\n",
        "\n",
        "      - Student Performance Prediction: Determine students at risk of failure.\n",
        "\n",
        "      - Admissions Decisions: Evaluate applicant success probability.\n",
        "\n",
        "  5. Manufacturing\n",
        "\n",
        "     - Quality Control: Detect defects in products based on features.\n",
        "\n",
        "     - Process Optimization: Identify key factors affecting production efficiency.\n",
        "\n",
        "- Advantages of Decision Trees\n",
        "\n",
        "  - Easy to understand\n",
        "  - No need for data scaling\n",
        "  - Handles non-linear relationship\n",
        "  - Features importance\n",
        "  - Fast and furious\n",
        "\n",
        "- Limitations of Decision Trees\n",
        "\n",
        "  - Overfitting\n",
        "  - Unstable\n",
        "  - Biased toward features with many levels\n",
        "  - Not ideal for continuous relations\n",
        "\n",
        "#Dataset Info:\n",
        "● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).\n",
        "● Boston Housing Dataset for regression tasks\n",
        "(sklearn.datasets.load_boston() or provided CSV).\n",
        "#Question 6: Write a Python program to:\n",
        "#● Load the Iris Dataset\n",
        "#● Train a Decision Tree Classifier using the Gini criterion\n",
        "#● Print the model’s accuracy and feature importances  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oR_xSZ-IrIsk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAOT60kRUyYO",
        "outputId": "59b4bdf2-941b-465e-a631-4bf5b7377a18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 7: Write a Python program to:\n",
        "#● Load the Iris Dataset\n",
        "#● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree."
      ],
      "metadata": {
        "id": "7lYjWCZZskCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "tree_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "tree_limited.fit(X_train, y_train)\n",
        "y_pred_limited = tree_limited.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "tree_full = DecisionTreeClassifier(random_state=42)\n",
        "tree_full.fit(X_train, y_train)\n",
        "y_pred_full = tree_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "print(\"Accuracy with max_depth=3:\", accuracy_limited)\n",
        "print(\"Accuracy with fully-grown tree:\", accuracy_full)\n",
        "\n",
        "print(\"\\nAccuracy Difference:\", round(accuracy_full - accuracy_limited, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wjyyg5tzseaZ",
        "outputId": "38c1d41e-9a65-47c9-dc8c-26b67cd46a52"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 1.0\n",
            "Accuracy with fully-grown tree: 1.0\n",
            "\n",
            "Accuracy Difference: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 8: Write a Python program to:\n",
        "#● Load the Boston Housing Dataset\n",
        "#● Train a Decision Tree Regressor\n",
        "#● Print the Mean Squared Error (MSE) and feature importances"
      ],
      "metadata": {
        "id": "9O9QYbsztgeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "y_pred = regressor.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(housing.feature_names, regressor.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFRs_rsyuKZY",
        "outputId": "489cd8e2-2102-43c3-b084-c805d4d95cba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.495235205629094\n",
            "\n",
            "Feature Importances:\n",
            "MedInc: 0.5285\n",
            "HouseAge: 0.0519\n",
            "AveRooms: 0.0530\n",
            "AveBedrms: 0.0287\n",
            "Population: 0.0305\n",
            "AveOccup: 0.1308\n",
            "Latitude: 0.0937\n",
            "Longitude: 0.0829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 9: Write a Python program to:\n",
        "#● Load the Iris Dataset\n",
        "#● Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV\n",
        "#● Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "huKH1Eexuzxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 3, 4, 5, 6, 10]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c81PKJXlukAn",
        "outputId": "1402c67f-e428-4361-8ac2-f7153bfa3c59"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Test Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 10: Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values. Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "\n",
        "● Encode the categorical features\n",
        "\n",
        "● Train a Decision Tree model\n",
        "\n",
        "● Tune its hyperparameters\n",
        "\n",
        "● Evaluate its performance\n",
        "\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n"
      ],
      "metadata": {
        "id": "GbJhyuK1vqve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer 10 - :\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Step-by-Step Process for Disease Prediction Using Decision Tree\n",
        "\n",
        "1. Handle Missing Values\n",
        "\n",
        "   - For numeric columns: replace missing values with median or mean using SimpleImputer.\n",
        "\n",
        "   - For categorical columns: fill missing values with \"MISSING\" or most frequent category.\n",
        "\n",
        "2. Encode Categorical Features\n",
        "\n",
        "   - Use OneHotEncoder for nominal data\n",
        "   - Use OrdinalEncoder if categories have order\n",
        "\n",
        "3. Train the Decision Tree Model\n",
        "\n",
        "4. Tune Hyperparameters\n",
        "\n",
        "   - max_depth\n",
        "\n",
        "   - min_samples_split\n",
        "\n",
        "   - min_samples_leaf\n",
        "\n",
        "   - criterion\n",
        "\n",
        "5. Evaluate the Model\n",
        "\n",
        "  - Accuracy, Precision, Recall, F1-score, and ROC-AUC.\n",
        "\n",
        "  - Confusion matrix to check false positives and negatives.\n",
        "\n",
        "6. Business Value\n",
        "\n",
        "  - Helps detect diseases early → saves lives.\n",
        "\n",
        "  - Supports doctors in decision-making.\n",
        "\n"
      ],
      "metadata": {
        "id": "9jlZb8XExoXS"
      }
    }
  ]
}